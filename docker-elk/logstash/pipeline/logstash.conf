input {
        beats {
                port => 5044
        }

        tcp {
                port => 50000
        }
        kafka {
                bootstrap_servers => "192.168.1.76:9092"
                topics => ["brain_data"]
                group_id => "logstash-consumer-group"
        }
}

filter {
    csv {
        separator => "," # Set the separator to comma
        columns => ["ID","Task Identifier","Activity Type","Repetition","Subject","Electrode 1","Electrode 2","Electrode 3","Electrode 4","Electrode 5","Electrode 6","Electrode 7","Electrode 8","Electrode 9","Electrode 10","Electrode 11","Electrode 12","Electrode 13","Electrode 14","Electrode 15","Electrode 16","ts"]
        source => "message" # Specify the source field
        convert => {
                "ID" => "integer"
		"Repetition" => "integer"
		"Subject" => "integer"
		"Electrode 1" => "float"
		"Electrode 2" => "float"
		"Electrode 3" => "float"
		"Electrode 4" => "float"
		"Electrode 5" => "float"
		"Electrode 6" => "float"
		"Electrode 7" => "float"
		"Electrode 8" => "float"
		"Electrode 9" => "float"
		"Electrode 10" => "float"
		"Electrode 11" => "float"
		"Electrode 12" => "float"
		"Electrode 13" => "float"
		"Electrode 14" => "float"
		"Electrode 15" => "float"
		"Electrode 16" => "float"
        }
    }
    
    # Parse timestamp from the CSV data if available
    date {
        match => ["timestamp", "dd-MM-yyyy HH:mm:ss"] # Adjust format as needed
        target => "timestamp"
    }
}

## Add your filters / logstash plugins configuration here

output {
        elasticsearch {
                hosts => "elasticsearch:9200"
                user => "logstash_internal"
                password => "${LOGSTASH_INTERNAL_PASSWORD}"
                ssl => true
                cacert => "config/ca.crt"
        }
}
